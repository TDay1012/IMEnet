import math
import torch
from torch import nn
from torch.nn import Parameter
import torch.nn.functional as F
import numpy as np


# 创建邻接矩阵
class get_adj(nn.Module):
    def __init__(self, N, J, device='cuda'):
        super(get_adj, self).__init__()
        self.n_person = N
        self.n_join = J
        self.device = device

    def forward(self, edges):
        J = self.n_join
        device = self.device
        adj = np.eye(J)
        for edge in edges:
            adj[edge[0], edge[1]] = 1
            adj[edge[1], edge[0]] = 1

        if device == 'cuda':
            return torch.from_numpy(adj).float().cuda()
        else:
            return torch.from_numpy(adj).float().cpu()

class GraphConvolution(nn.Module):
    """
    adapted from : https://github.com/tkipf/gcn/blob/92600c39797c2bfb61a508e52b88fb554df30177/gcn/layers.py#L132
    """
    def __init__(self, in_features, out_features, bias=True, node_n=48,device='cuda'):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        self.att = Parameter(torch.FloatTensor(node_n, node_n))
        # 创建邻接矩阵
        self.edges = [[0, 1], [1, 2], [2, 3], [0, 4],[4, 5], [5, 6], [0, 7], [7, 8], [7, 9], [9, 10], [10, 11], [7, 12], [12, 13], [13, 14]]
        self.get_adj = get_adj(N=3, J=15,device=device)

        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        self.att.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input):
        # adj = self.get_adj(self.edges)
        support = torch.matmul(input, self.weight)  # w:
        output = torch.matmul(self.att, support)  # att:新加 adj

        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

class GCN_Block(nn.Module):
    def __init__(self, input_feature, hidden_feature, node_n=48,device='cuda'):
        """
        :param input_feature: num of input feature
        :param hidden_feature: num of hidden feature
        :param p_dropout: drop out prob.
        :param num_stage: number of residual blocks
        :param node_n: number of nodes in graph
        """
        super(GCN_Block, self).__init__()
        self.gc1 = GraphConvolution(input_feature, input_feature, node_n=node_n,device=device)
        self.bn1 = nn.BatchNorm1d(node_n * input_feature)

        self.gc2 = GraphConvolution(input_feature, 128, node_n=node_n,device=device)
        self.bn2 = nn.BatchNorm1d(node_n * 128)

        self.gc3 = GraphConvolution(128, 512, node_n=node_n,device=device)
        self.bn3 = nn.BatchNorm1d(node_n * 512)

        self.gc4 = GraphConvolution(512, 128, node_n=node_n,device=device)
        self.bn4 = nn.BatchNorm1d(node_n * 128)

        self.gc5 = GraphConvolution(128, hidden_feature, node_n=node_n,device=device)
        self.bn5 = nn.BatchNorm1d(node_n * hidden_feature)

        self.gc6 = GraphConvolution(hidden_feature, hidden_feature, node_n=node_n,device=device)
        self.bn6 = nn.BatchNorm1d(node_n * hidden_feature)

        self.do = nn.Dropout(0.3)
        self.act_f = nn.Tanh()

    def forward(self, x):
        y = self.gc1(x)
        b, n, f = y.shape
        y = self.bn1(y.view(b, -1)).view(b, n, f)
        y = self.act_f(y)
        y = self.do(y)
        y = y + x

        y = self.gc2(y)
        y = self.gc3(y)
        y = self.gc4(y)
        y = self.gc5(y)

        y = self.gc6(y)
        b, n, f = y.shape
        y = self.bn6(y.view(b, -1)).view(b, n, f)
        y = self.act_f(y)
        y = self.do(y)
        y = y + x

        return y


class GCN_Layer(nn.Module):
    def __init__(self, input_feature, hidden_feature, node_n=48):
        """
        :param input_feature: num of input feature
        :param hidden_feature: num of hidden feature
        :param p_dropout: drop out prob.
        :param num_stage: number of residual blocks
        :param node_n: number of nodes in graph
        """
        super(GCN_Layer, self).__init__()
        self.gc1 = GraphConvolution(input_feature, hidden_feature, node_n=node_n)
        self.bn1 = nn.BatchNorm1d(node_n * hidden_feature)

        self.gc2 = GraphConvolution(4, 16, node_n=node_n)
        self.bn2 = nn.BatchNorm1d(node_n * 16)

        self.gc3 = GraphConvolution(16, hidden_feature, node_n=node_n)
        self.bn3 = nn.BatchNorm1d(node_n * hidden_feature)

        self.do = nn.Dropout(0.3)
        self.act_f = nn.Tanh()

    def forward(self, x, adj_ture=False):

        y = self.gc1(x, adj_ture)
        b, n, f = y.shape
        y = self.bn1(y.view(b, -1)).view(b, n, f)
        y = self.act_f(y)
        y = self.do(y)

        return y
